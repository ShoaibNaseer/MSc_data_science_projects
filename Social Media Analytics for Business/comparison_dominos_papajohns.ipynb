{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26116,
     "status": "ok",
     "timestamp": 1701483912366,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "A-9S8n3IkRjl",
    "outputId": "3a00a304-2be5-44f6-aef9-2190141ef022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1701483912929,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "ah_oPPM3kb1p",
    "outputId": "b7d554e9-7dc9-4224-d711-5ff5c86735c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/sma_colab/final_assessment\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/My Drive/sma_colab/final_assessment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0e-3WJj95ws"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJHT0LaJkeK5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.selector import Selector\n",
    "import json\n",
    "from html import unescape\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5K5ByWltqM0"
   },
   "source": [
    "# **Function to make folder if it does not exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGhdybErgvRs"
   },
   "outputs": [],
   "source": [
    "def create_folder(folder_path):\n",
    "  if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_7bu7635_A4"
   },
   "outputs": [],
   "source": [
    "# Create parent folder of dominos and papa johns\n",
    "create_folder(\"dominos\")\n",
    "create_folder(\"papa_johns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doT_Lr3yi6zp"
   },
   "outputs": [],
   "source": [
    "# Create sub folders\n",
    "\n",
    "create_folder(\"dominos/trust_pilot\")\n",
    "create_folder(\"dominos/consumer_affairs\")\n",
    "\n",
    "create_folder(\"papa_johns/trust_pilot\")\n",
    "create_folder(\"papa_johns/consumer_affairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8MPMYdXt0ye"
   },
   "source": [
    "# **Method to download html pages of consumer affairs and trust pilot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv8Q-8gtkSbM"
   },
   "outputs": [],
   "source": [
    "def download_pages(url_link, folder_name, end_loop = 101, url_end = None):\n",
    "  for page_no in range(1, end_loop):\n",
    "    url_link = url_link + str(page_no) + (url_end if url_end is not None else \"\")\n",
    "    response = requests.get(url_link)\n",
    "    file_name = folder_name + \"page\" + str(page_no) + \".html\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "      file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d90U78q6kwZe"
   },
   "outputs": [],
   "source": [
    "download_pages(\"https://uk.trustpilot.com/review/www.dominos.co.uk?page=\", \"dominos/trust_pilot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BxGSKuCmhYo"
   },
   "outputs": [],
   "source": [
    "download_pages(\"https://uk.trustpilot.com/review/www.papajohns.co.uk?page=\", \"papa_johns/trust_pilot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t16w9WbcnBv8"
   },
   "outputs": [],
   "source": [
    "download_pages(\"https://www.consumeraffairs.com/food/dominos.html?page=\", \"dominos/consumer_affairs/\", 6, \"#scroll_to_reviews=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAoydkHpnCQd"
   },
   "outputs": [],
   "source": [
    "download_pages(\"https://www.consumeraffairs.com/food/papa_johns.html?page=\", \"papa_johns/consumer_affairs/\", 6, \"#scroll_to_reviews=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9L0VuxmNwth"
   },
   "source": [
    "# **Extracting Trust Pilot Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjcZPsZIuQw3"
   },
   "outputs": [],
   "source": [
    "def extract_trust_pilot(company_name):\n",
    "  folder_path = company_name + '/trust_pilot'\n",
    "  file_list = os.listdir(folder_path)\n",
    "  extracted_data = []\n",
    "  for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "      file_content = file.read()\n",
    "      selector = Selector(text=file_content)\n",
    "      overall_data = selector.css('script[type=\"application/ld+json\"]::text').extract()[0]\n",
    "      jsons_data = json.loads(overall_data)['@graph']\n",
    "      for json_data in jsons_data:\n",
    "        if json_data['@type'] == 'Review':\n",
    "          # author_name = json_data['author']['name']\n",
    "          published_date = json_data['datePublished']\n",
    "          review = json_data['reviewBody']\n",
    "          rating = json_data['reviewRating']['ratingValue']\n",
    "          extracted_data.append([published_date, review.strip().encode('ascii', 'ignore').decode('ascii'), rating, company_name, \"trust_pilot\"])\n",
    "  return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Icll_UCzLVT"
   },
   "outputs": [],
   "source": [
    "dominos_reviews_tp = extract_trust_pilot(\"dominos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awxzunAHA8Q3"
   },
   "outputs": [],
   "source": [
    "papa_johns_reviews_tp = extract_trust_pilot(\"papa_johns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3zmmc9vN7Ew"
   },
   "source": [
    "# **Extracting Consumer Affairs Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyA4blbeN-Ez"
   },
   "outputs": [],
   "source": [
    "def extract_consumer_affair(company_name):\n",
    "  folder_path = company_name + '/consumer_affairs'\n",
    "  file_list = os.listdir(folder_path)\n",
    "  extracted_data = []\n",
    "  for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "      file_content = file.read()\n",
    "      selector = Selector(text=file_content)\n",
    "      overall_data_date = selector.css(\"#reviews-container .rvw__dtls .rvw__rvd-dt::text\").extract()\n",
    "      overall_data_reviews = selector.css(\"#reviews-container .rvw__dtls .rvw__bd p::text\").extract()\n",
    "      # overall_first_name = selector.css(\".rvw__inf span:nth-child(1)::text\").extract()\n",
    "      # overall_second_name = selector.css(\".rvw__inf span:nth-child(2)::text\").extract()\n",
    "      for date, review in zip(overall_data_date, overall_data_reviews):\n",
    "        extracted_data.append([date.replace(\"Reviewed\", \"\").strip(), review.strip().encode('ascii', 'ignore')\n",
    "        .decode('ascii'), None, company_name, \"consumer_affairs\"])\n",
    "  return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQEn-_rxP5DW"
   },
   "outputs": [],
   "source": [
    "dominos_reviews_ca = extract_consumer_affair(\"dominos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXggKO4kQFPd"
   },
   "outputs": [],
   "source": [
    "papa_johns_reviews_ca = extract_consumer_affair(\"papa_johns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sZOpv8NQK7W"
   },
   "source": [
    "# **Save CSV Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAchKULL1Faq"
   },
   "outputs": [],
   "source": [
    "def save_csv(extracted_data, company_name):\n",
    "  csv_file_path = company_name + '_reviews.csv'\n",
    "  with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header row\n",
    "    csv_writer.writerow(['published_date', 'review', 'rating', 'company', 'source'])\n",
    "\n",
    "    # Write the extracted data\n",
    "    csv_writer.writerows(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1700962635594,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "I7IHWtbg005H",
    "outputId": "b0989529-9c6b-4862-b137-7034286ad4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written\n"
     ]
    }
   ],
   "source": [
    "save_csv(dominos_reviews_tp, \"dominos_tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1700962673047,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "DBrUZA9iHOEg",
    "outputId": "61a1fd8c-9ea4-44d1-f1bf-652314fed004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written\n"
     ]
    }
   ],
   "source": [
    "save_csv(papa_johns_reviews_tp, \"papa_johns_tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1700962700972,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "Tvm5fEQLQQIS",
    "outputId": "ca928b49-c63c-4e78-aa3f-be05257bb1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written\n"
     ]
    }
   ],
   "source": [
    "save_csv(dominos_reviews_ca, \"dominos_ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1700962719903,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "6Jul7wwsQP7E",
    "outputId": "03fd798f-8365-4290-d6aa-ea3ac9264a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written\n"
     ]
    }
   ],
   "source": [
    "save_csv(papa_johns_reviews_ca, \"papa_johns_ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypkn0jnmbIf5"
   },
   "source": [
    "# **Sentimental Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUQlRNduDZfI"
   },
   "outputs": [],
   "source": [
    "nrc_lexicon = pd.read_csv('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', names=['word', 'emotion', 'association'], skiprows=45, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1VmCjg79FA_"
   },
   "outputs": [],
   "source": [
    "def word_cloud(review):\n",
    "  tokens = word_tokenize(review)\n",
    "\n",
    "  # Remove stopwords\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [word.lower() for word in tokens if word.lower() not in stop_words and len(word) >= 3]\n",
    "\n",
    "  # Lemmatize\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "  # Filter lexicon for relevant words\n",
    "  # Assuming nrc_lexicon is loaded earlier in your code\n",
    "  filtered_lexicon = nrc_lexicon[nrc_lexicon['word'].isin(tokens)]\n",
    "\n",
    "  # Display emotions associated with each word\n",
    "  return sorted(filtered_lexicon[['word', 'emotion']]['word'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrXk8oh2bbna"
   },
   "outputs": [],
   "source": [
    "def sentimental_analysis(csv_files):\n",
    "  sentiments = []\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('averaged_perceptron_tagger')\n",
    "  nltk.download('brown')\n",
    "  nltk.download('stopwords')\n",
    "  nltk.download('wordnet')\n",
    "  for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Perform sentiment analysis on each row\n",
    "    for index, row in df.iterrows():\n",
    "      text = row['review']\n",
    "      blob = TextBlob(str(text))\n",
    "      sentiment_polarity = blob.sentiment.polarity\n",
    "      sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "      pos_tags = blob.tags\n",
    "      nouns = ', '.join(blob.noun_phrases)\n",
    "      sentence_words = ', '.join(blob.words)\n",
    "\n",
    "      cleaned_text = word_cloud(row['review'])\n",
    "      sentiments.append({'published_date': row['published_date'], 'review': row['review'], 'rating': row['rating'], 'company': row['company'],\n",
    "                         'source': row['source'], 'Text': text, 'Sentiment Polarity': sentiment_polarity,\n",
    "                         'Sentiment Subjectivity': sentiment_subjectivity, 'pos_tags': pos_tags, 'nouns': nouns, 'sentence_words': sentence_words,\n",
    "                         'cleaned_text': cleaned_text})\n",
    "\n",
    "  # Create a new DataFrame with sentiment analysis results\n",
    "  sentiments_df = pd.DataFrame(sentiments)\n",
    "\n",
    "  # Save the results to a new CSV file\n",
    "  output_csv_path = 'sentiment_analysis_results.csv'\n",
    "  sentiments_df.to_csv(output_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69534,
     "status": "ok",
     "timestamp": 1701486645101,
     "user": {
      "displayName": "Shoaib Naseer",
      "userId": "04232670550818350272"
     },
     "user_tz": 0
    },
    "id": "hvkRT_aDlCSb",
    "outputId": "59512b62-a255-44b8-9fba-69370254976d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sentimental_analysis([\"papa_johns_tp_reviews.csv\", \"dominos_ca_reviews.csv\", \"dominos_tp_reviews.csv\", \"papa_johns_ca_reviews.csv\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDc4mqEXCRmlx1IfWICh9O",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
